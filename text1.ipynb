{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abab241d-c626-49a7-9f64-d340a12c1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import math\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdc37e04-2009-4500-b7e2-a15c78fef9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.8/3.8 MB 16.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.1/3.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 7.6 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0653f6b3-e483-4789-a45e-dbedefd5f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sayani Roy\n",
      "[nltk_data]     Choudhury\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Execute this line if you are running this code for first time\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Initializing few variable\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "#Step 2. Define functions for Reading Input Text\n",
    "\n",
    "#Function to Read .txt File and return its Text\n",
    "def file_text(filepath):\n",
    "    with open(filepath) as f:\n",
    "        text = f.read().replace(\"\\n\", '')\n",
    "        return text\n",
    "\n",
    "\n",
    "#Function to Read PDF File and return its Text\n",
    "def pdfReader(pdf_path):\n",
    "    \n",
    "    with open(pdf_path, 'rb') as pdfFileObject:\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "        count = pdfReader.numPages\n",
    "        print(\"\\nTotal Pages in pdf = \", count)\n",
    "        \n",
    "        c = 'Y'\n",
    "        start_page = 0\n",
    "        end_page = count-1\n",
    "        c = input(\"Do you want to read entire pdf ?[Y]/N  :  \")\n",
    "        if c == 'N' or c == 'n' :\n",
    "            start_page  = int(input(\"Enter start page number (Indexing start from 0) :  \"))\n",
    "            end_page = int(input(f\"Enter end page number (Less than {count}) : \"))\n",
    "            \n",
    "            if start_page <0 or start_page >= count:\n",
    "                print(\"\\nInvalid Start page given\")\n",
    "                sys.exit()\n",
    "                \n",
    "            if end_page <0 or end_page >= count:\n",
    "                print(\"\\nInvalid End page given\")\n",
    "                sys.exit()\n",
    "                \n",
    "        for i in range(start_page,end_page+1):\n",
    "            page = pdfReader.getPage(i)\n",
    "\n",
    "        return page.extractText()\n",
    "    \n",
    "    \n",
    "#Function to Read wikipedia page url and return its Text   \n",
    "def wiki_text(url):\n",
    "    scrap_data = urllib.request.urlopen(url)\n",
    "    article = scrap_data.read()\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "    \n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    article_text = \"\"\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    \n",
    "    #Removing all unwanted characters\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', '', article_text)\n",
    "    return article_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10bffde6-cac3-45ba-bf7a-9e61d3f75500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select one way of inputting your text : \n",
      "1. Type your Text(or Copy-Paste)\n",
      "2. Load from .txt file\n",
      "3. Load from .pdf file\n",
      "4. From Wikipedia Page URL\n",
      "\n",
      " 4\n",
      "Enter Wikipedia URL to load Article :  https://en.wikipedia.org/wiki/Artificial_intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lxml parser not found, using html.parser as a fallback.\n"
     ]
    }
   ],
   "source": [
    "#Step 3. Getting Text \n",
    "\n",
    "input_text_type = int(input(\"Select one way of inputting your text \\\n",
    ": \\n1. Type your Text(or Copy-Paste)\\n2. Load from .txt file\\n3. Load from .pdf file\\n4. From Wikipedia Page URL\\n\\n\"))\n",
    "\n",
    "if input_text_type == 1:\n",
    "    text = input(u\"Enter your text : \\n\\n\")\n",
    "\n",
    "elif input_text_type == 2:\n",
    "    txt_path = input(\"Enter file path :  \")\n",
    "    text = file_text(txt_path)\n",
    "    \n",
    "elif input_text_type == 3:\n",
    "    file_path = input(\"Enter file path :  \")\n",
    "    text = pdfReader(file_path)\n",
    "    \n",
    "elif input_text_type == 4:\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup, FeatureNotFound  # Explicitly import FeatureNotFound\n",
    "\n",
    "    wiki_url = input(\"Enter Wikipedia URL to load Article : \")\n",
    "    \n",
    "    scrap_data = urllib.request.urlopen(wiki_url)\n",
    "    article = scrap_data.read()\n",
    "\n",
    "    # Try to use 'lxml' parser, and fall back to 'html.parser' if lxml is unavailable\n",
    "    try:\n",
    "        parsed_article = BeautifulSoup(article, 'lxml')  # Attempt to use lxml parser\n",
    "    except FeatureNotFound:\n",
    "        print(\"lxml parser not found, using html.parser as a fallback.\")\n",
    "        parsed_article = BeautifulSoup(article, 'html.parser')  # Fallback to html.parser\n",
    "\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    text = \" \".join([para.get_text() for para in paragraphs])\n",
    "\n",
    "else:\n",
    "    print(\"Sorry! Wrong Input, Try Again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51884d13-1af2-44ae-85cd-aafa08d4ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4. Defining functions to create Tf-Idf Matrix\n",
    "\n",
    "\n",
    "#Function to calculate frequency of word in each sentence\n",
    "#INPUT -> List of all sentences from text as spacy.Doc object\n",
    "#OUTPUT -> freq_matrix (A dictionary with each sentence itself as key, \n",
    "# and a dictionary of words of that sentence with their frequency as value)\n",
    "\n",
    "def frequency_matrix(sentences):\n",
    "    freq_matrix = {}\n",
    "    stopWords = nlp.Defaults.stop_words\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {} #dictionary with 'words' as key and their 'frequency' as value\n",
    "        \n",
    "        #Getting all word from the sentence in lower case\n",
    "        words = [word.text.lower() for word in sent  if word.text.isalnum()]\n",
    "       \n",
    "        for word in words:  \n",
    "            word = lemmatizer.lemmatize(word)   #Lemmatize the word\n",
    "            if word not in stopWords:           #Reject stopwords\n",
    "                if word in freq_table:\n",
    "                    freq_table[word] += 1\n",
    "                else:\n",
    "                    freq_table[word] = 1\n",
    "\n",
    "        freq_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return freq_matrix\n",
    "\n",
    "\n",
    "#Function to calculate Term Frequency(TF) of each word\n",
    "#INPUT -> freq_matrix\n",
    "#OUTPUT -> tf_matrix (A dictionary with each sentence itself as key, \n",
    "# and a dictionary of words of that sentence with their Term-Frequency as value)\n",
    "\n",
    "#TF(t) = (Number of times term t appears in  document) / (Total number of terms in the document)\n",
    "def tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, freq_table in freq_matrix.items():\n",
    "        tf_table = {}  #dictionary with 'word' itself as a key and its TF as value\n",
    "\n",
    "        total_words_in_sentence = len(freq_table)\n",
    "        for word, count in freq_table.items():\n",
    "            tf_table[word] = count / total_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "#Function to find how many sentences contain a 'word'\n",
    "#INPUT -> freq_matrix\n",
    "#OUTPUT -> sent_per_words (Dictionary with each word itself as key and number of \n",
    "#sentences containing that word as value)\n",
    "\n",
    "def sentences_per_words(freq_matrix):\n",
    "    sent_per_words = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in sent_per_words:\n",
    "                sent_per_words[word] += 1\n",
    "            else:\n",
    "                sent_per_words[word] = 1\n",
    "\n",
    "    return sent_per_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf930b77-57ff-4c86-9f2b-1208e4d98c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Inverse Document frequency(IDF) for each word\n",
    "#INPUT -> freq_matrix,sent_per_words, total_sentences\n",
    "#OUTPUT -> idf_matrix (A dictionary with each sentence itself as key, \n",
    "# and a dictionary of words of that sentence with their IDF as value)\n",
    "\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "def idf_matrix(freq_matrix, sent_per_words, total_sentences):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_sentences / float(sent_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c81dcde3-cc19-492a-9dac-dde6f451474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Tf-Idf score of each word\n",
    "#INPUT -> tf_matrix, idf_matrix\n",
    "#OUTPUT - > tf_idf_matrix (A dictionary with each sentence itself as key, \n",
    "# and a dictionary of words of that sentence with their Tf-Idf as value)\n",
    "def tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "       #word1 and word2 are same\n",
    "        for (word1, tf_value), (word2, idf_value) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  \n",
    "            tf_idf_table[word1] = float(tf_value * idf_value)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc847d52-bd3a-4c30-9428-86c72a1633da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to rate every sentence with some score calculated on basis of Tf-Idf\n",
    "#INPUT -> tf_idf_matrix\n",
    "#OUTPUT - > sentenceScore (Dictionary with each sentence itself as key and its score\n",
    "# as value)\n",
    "def score_sentences(tf_idf_matrix):\n",
    "    \n",
    "    sentenceScore = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_tfidf_score_per_sentence = 0\n",
    "\n",
    "        total_words_in_sentence = len(f_table)\n",
    "        for word, tf_idf_score in f_table.items():\n",
    "            total_tfidf_score_per_sentence += tf_idf_score\n",
    "\n",
    "        if total_words_in_sentence != 0:\n",
    "            sentenceScore[sent] = total_tfidf_score_per_sentence / total_words_in_sentence\n",
    "\n",
    "    return sentenceScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db04308d-8398-426c-9715-3eaf61764d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Calculating average sentence score \n",
    "#INPUT -> sentence_score\n",
    "#OUTPUT -> average_sent_score(An average of the sentence_score) \n",
    "def average_score(sentence_score):\n",
    "    \n",
    "    total_score = 0\n",
    "    for sent in sentence_score:\n",
    "        total_score += sentence_score[sent]\n",
    "\n",
    "    average_sent_score = (total_score / len(sentence_score))\n",
    "\n",
    "    return average_sent_score\n",
    "\n",
    "\n",
    "#Function to return summary of article\n",
    "#INPUT -> sentences(list of all sentences in article), sentence_score, threshold\n",
    "# (set to the average pf sentence_score)\n",
    "#OUTPUT -> summary (String text)\n",
    "def create_summary(sentences, sentence_score, threshold):\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentence_score and sentence_score[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence.text\n",
    "        \n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1b8956b-0228-4156-a457-0017bd7ec281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "******************** Summary ********************\n",
      "\n",
      "\n",
      " They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n",
      "  A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n",
      "  In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. A policy associates a decision with each possible state. There are two very different kinds of search used in AI: state space search and local search.\n",
      "  State space search searches through a tree of possible states to try to find a goal state.[69] Other specialized versions of logic have been developed to describe many complex domains.\n",
      "  information value theory.[88] Classifiers[98] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n",
      " There are many kinds of classifiers in use.[99] It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. The multiple layers can progressively extract higher-level features from the raw input. The reason that deep learning performs so well in so many applications is not known as of 2023.[114] Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). The deployment of AI may be overseen by a Chief automation officer (CAO).\n",
      "  New AI tools can deepen the understanding of biomedically relevant pathways. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[141] Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[145] \"[154]\n",
      " Various countries are deploying AI military applications.[155] The main applications enhance command and control, communications, sensors, integration and interoperability.[156] These agents can interact with users, their environment, or other agents. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[168] AI applications for evacuation and disaster management are growing. Agronomists use AI to conduct research and development. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n",
      "  Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. Closed since 2022, the plant is planned to be reopened in October 2025. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[203] The developers may not be aware that the bias exists.[207] The field of fairness studies how to prevent harms from algorithmic biases.\n",
      "  The system was trained on a dataset that contained very few images of black people,[210] a problem called \"sample size disparity\".[211] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[216] If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[218] These notions depend on ethical assumptions, and are influenced by beliefs about society. Procedural fairness focuses on the decision process rather than the outcome. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Several approaches aim to address the transparency problem. Face and voice recognition allow widespread surveillance. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. It lowers the cost and difficulty of digital warfare and advanced spyware.[237] This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[251] These sci-fi scenarios are misleading in several ways.\n",
      "  The essential parts of civilization are not physical. While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[314] In 1967 Marvin Minsky agreed, writing that \"within a generationÂ ... the problem of creating 'artificial intelligence' will substantially be solved\".[315] They had, however, underestimated the difficulty of the problem.[w] By 1985, the market for AI had reached over a billion dollars. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[342] Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks. However, they are critical that the test requires the machine to imitate humans. Another definition has been adopted by Google,[349] a major practitioner in the field of AI. This approach is mostly sub-symbolic, soft and narrow. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. This issue was actively discussed in the 1970s and 1980s,[360] but eventually was seen as irrelevant. Modern AI has elements of both.\n",
      "  This issue considers the internal experiences of the machine, rather than its external behavior. However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n",
      "  David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[364] While human information processing is easy to explain, human subjective experience is difficult to explain. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[375] This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001:\n",
      "\n",
      "\n",
      "\n",
      "Total words in original article =  10313\n",
      "Total words in summarized article =  1100\n"
     ]
    }
   ],
   "source": [
    "#Step 5. Using all functions to generate summary\n",
    "\n",
    "\n",
    "#Counting number of words in original article\n",
    "original_words = text.split()\n",
    "original_words = [w for w in original_words if w.isalnum()]\n",
    "num_words_in_original_text = len(original_words)\n",
    "\n",
    "\n",
    "#Converting received text into sapcy Doc object\n",
    "text = nlp(text)\n",
    "\n",
    "#Extracting all sentences from the text in a list\n",
    "sentences = list(text.sents)\n",
    "total_sentences = len(sentences)\n",
    "\n",
    "#Generating Frequency Matrix\n",
    "freq_matrix = frequency_matrix(sentences)\n",
    "\n",
    "#Generating Term Frequency Matrix\n",
    "tf_matrix = tf_matrix(freq_matrix)\n",
    "\n",
    "#Getting number of sentences containing a particular word\n",
    "num_sent_per_words = sentences_per_words(freq_matrix)\n",
    "\n",
    "#Generating ID Frequency Matrix\n",
    "idf_matrix = idf_matrix(freq_matrix, num_sent_per_words, total_sentences)\n",
    "\n",
    "#Generating Tf-Idf Matrix\n",
    "tf_idf_matrix = tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "\n",
    "\n",
    "#Generating Sentence score for each sentence\n",
    "sentence_scores = score_sentences(tf_idf_matrix)\n",
    "\n",
    "#Setting threshold to average value (You are free to play with ther values) \n",
    "threshold = average_score(sentence_scores)\n",
    "\n",
    "#Getting summary \n",
    "summary = create_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "print(\"\\n\\n\")\n",
    "print(\"*\"*20,\"Summary\",\"*\"*20)\n",
    "print(\"\\n\")\n",
    "print(summary)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Total words in original article = \", num_words_in_original_text)\n",
    "print(\"Total words in summarized article = \", len(summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41779c6e-915c-4f4c-9a93-7dc5759384a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe12805-385f-4f24-8945-8ac4a304bee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
